{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1. What is a Support Vector Machine (SVM), and how does it work?\n",
        "  - Support Vector Machine (SVM) is a supervised learning algorithm used for classification and regression. It works by finding the optimal hyperplane that best separates different classes in the feature space. The goal is to maximize the margin (distance) between the hyperplane and the nearest data points (called support vectors).\n",
        "\n",
        "2. Explain the difference between Hard Margin and Soft Margin SVM.\n",
        "  - Hard Margin SVM: Assumes data is perfectly linearly separable. It requires all points to be correctly classified without error. Not practical for noisy data.\n",
        "\n",
        "  - Soft Margin SVM: Allows some misclassifications (controlled by parameter C). Balances maximizing margin with minimizing classification error, making it more robust to noise.\n",
        "\n",
        "3. What is the Kernel Trick in SVM? Give one example of a kernel and\n",
        "explain its use case.\n",
        "  - The Kernel Trick allows SVM to classify data that is not linearly separable by implicitly mapping it into a higher-dimensional space without explicitly computing the transformation.\n",
        "\n",
        "  - Example: Radial Basis Function (RBF) Kernel\n",
        "Use case: When the data has non-linear decision boundaries (e.g., circular clusters).\n",
        "\n",
        "4. What is a Naïve Bayes Classifier, and why is it called “naïve”?\n",
        "  - Naïve Bayes is a probabilistic classifier based on Bayes’ Theorem that assumes all features are independent given the class label.\n",
        "  - It’s called “naïve” because in reality, features are rarely completely independent, but the assumption simplifies computation and often works well in practice.\n",
        "\n",
        "5. Describe the Gaussian, Multinomial, and Bernoulli Naïve Bayes variants.\n",
        "When would you use each one?\n",
        "  - Gaussian NB: Assumes features follow a normal distribution. Used for continuous data (e.g., Iris dataset).\n",
        "\n",
        "  - Multinomial NB: Used for discrete counts (e.g., word counts in text classification).\n",
        "\n",
        "  - Bernoulli NB: Assumes binary features (0/1 presence). Useful for text classification with binary indicators of word presence.\n"
      ],
      "metadata": {
        "id": "YU8TPjt5nbew"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Dataset Info:\n",
        "● You can use any suitable datasets like Iris, Breast Cancer, or Wine from\n",
        "sklearn.datasets or a CSV file you have.\n",
        "Question 6: Write a Python program to:\n",
        "● Load the Iris dataset\n",
        "● Train an SVM Classifier with a linear kernel\n",
        "● Print the model's accuracy and support vectors.\n",
        "\"\"\"\n",
        "\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "iris = datasets.load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train SVM with linear kernel\n",
        "svm_model = SVC(kernel='linear')\n",
        "svm_model.fit(X_train, y_train)\n",
        "\n",
        "# Predictions and accuracy\n",
        "y_pred = svm_model.predict(X_test)\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(\"Support Vectors:\", svm_model.support_vectors_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MsfuPqKXotPE",
        "outputId": "91b35527-294a-4021-c8f7-1ba61af506f9"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 1.0\n",
            "Support Vectors: [[4.8 3.4 1.9 0.2]\n",
            " [5.1 3.3 1.7 0.5]\n",
            " [4.5 2.3 1.3 0.3]\n",
            " [5.6 3.  4.5 1.5]\n",
            " [5.4 3.  4.5 1.5]\n",
            " [6.7 3.  5.  1.7]\n",
            " [5.9 3.2 4.8 1.8]\n",
            " [5.1 2.5 3.  1.1]\n",
            " [6.  2.7 5.1 1.6]\n",
            " [6.3 2.5 4.9 1.5]\n",
            " [6.1 2.9 4.7 1.4]\n",
            " [6.5 2.8 4.6 1.5]\n",
            " [6.9 3.1 4.9 1.5]\n",
            " [6.3 2.3 4.4 1.3]\n",
            " [6.3 2.8 5.1 1.5]\n",
            " [6.3 2.7 4.9 1.8]\n",
            " [6.  3.  4.8 1.8]\n",
            " [6.  2.2 5.  1.5]\n",
            " [6.2 2.8 4.8 1.8]\n",
            " [6.5 3.  5.2 2. ]\n",
            " [7.2 3.  5.8 1.6]\n",
            " [5.6 2.8 4.9 2. ]\n",
            " [5.9 3.  5.1 1.8]\n",
            " [4.9 2.5 4.5 1.7]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Question 7: Write a Python program to:\n",
        "● Load the Breast Cancer dataset\n",
        "● Train a Gaussian Naïve Bayes model\n",
        "● Print its classification report including precision, recall, and F1-score.\n",
        "\"\"\"\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train Gaussian NB\n",
        "nb_model = GaussianNB()\n",
        "nb_model.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred = nb_model.predict(X_test)\n",
        "print(classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mCA6A79Xo4XL",
        "outputId": "d2ad5533-9ba9-4297-83fb-d1bdf750913d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.93      0.90      0.92        63\n",
            "           1       0.95      0.96      0.95       108\n",
            "\n",
            "    accuracy                           0.94       171\n",
            "   macro avg       0.94      0.93      0.94       171\n",
            "weighted avg       0.94      0.94      0.94       171\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Question 8: Write a Python program to:\n",
        "● Train an SVM Classifier on the Wine dataset using GridSearchCV to find the best\n",
        "C and gamma.\n",
        "● Print the best hyperparameters and accuracy.\n",
        "\"\"\"\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "wine = load_wine()\n",
        "X, y = wine.data, wine.target\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# GridSearch for best params\n",
        "param_grid = {'C': [0.1, 1, 10], 'gamma': [0.01, 0.1, 1]}\n",
        "grid = GridSearchCV(SVC(kernel='rbf'), param_grid, cv=5)\n",
        "grid.fit(X_train, y_train)\n",
        "\n",
        "# Results\n",
        "print(\"Best Parameters:\", grid.best_params_)\n",
        "y_pred = grid.predict(X_test)\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qJDeCqOppi82",
        "outputId": "f2c57d16-f37e-4708-be0a-a158284a5b9b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'C': 10, 'gamma': 0.01}\n",
            "Accuracy: 0.6666666666666666\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Question 9: Write a Python program to:\n",
        "● Train a Naïve Bayes Classifier on a synthetic text dataset (e.g. using\n",
        "sklearn.datasets.fetch_20newsgroups).\n",
        "● Print the model's ROC-AUC score for its predictions.\n",
        "\"\"\"\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "# Load dataset\n",
        "data = fetch_20newsgroups(subset='all')\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Vectorization\n",
        "vectorizer = TfidfVectorizer(stop_words='english')\n",
        "X_vec = vectorizer.fit_transform(X)\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_vec, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train NB\n",
        "nb_model = MultinomialNB()\n",
        "nb_model.fit(X_train, y_train)\n",
        "\n",
        "# ROC-AUC score\n",
        "y_prob = nb_model.predict_proba(X_test)\n",
        "print(\"ROC-AUC Score:\", roc_auc_score(y_test, y_prob, multi_class='ovr'))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "99E9HWIGpwqN",
        "outputId": "d69c91b2-4a32-4182-83bc-8ce4b0e3c6af"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROC-AUC Score: 0.9934558707675476\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. Question 10: Imagine you’re working as a data scientist for a company that handles\n",
        "email communications.\n",
        "Your task is to automatically classify emails as Spam or Not Spam. The emails may\n",
        "contain:\n",
        "● Text with diverse vocabulary\n",
        "● Potential class imbalance (far more legitimate emails than spam)\n",
        "● Some incomplete or missing data\n",
        "Explain the approach you would take to:\n",
        "● Preprocess the data (e.g. text vectorization, handling missing data)\n",
        "● Choose and justify an appropriate model (SVM vs. Naïve Bayes)\n",
        "● Address class imbalance\n",
        "● Evaluate the performance of your solution with suitable metrics\n",
        "And explain the business impact of your solution.\n",
        "\n",
        "#### Preprocessing\n",
        "- Handle missing data by replacing empty emails with placeholders (e.g., `\"no_content\"`).  \n",
        "- Use **TF-IDF vectorization** to convert text into numerical features.  \n",
        "- Normalize and clean text:  \n",
        "  - Remove stopwords  \n",
        "  - Remove punctuation and special characters  \n",
        "  - Convert to lowercase  \n",
        "\n",
        "\n",
        "###  Model Choice\n",
        "- **Naïve Bayes** (Multinomial NB) is efficient and effective for text classification, especially with word probability–based models.  \n",
        "- **SVM** can work better with high-dimensional sparse text features but is computationally more expensive.  \n",
        "- For a starting point, I’d use **Multinomial Naïve Bayes**.  \n",
        "\n",
        "\n",
        "\n",
        "###  Handling Class Imbalance\n",
        "- If spam emails are fewer than legitimate ones, balance the dataset by:  \n",
        "  - Using **SMOTE (Synthetic Minority Oversampling Technique)**  \n",
        "  - Applying **class weights** to penalize misclassification of minority class  \n",
        "\n",
        "\n",
        "\n",
        "### Evaluation Metrics\n",
        "- Use metrics beyond just accuracy:  \n",
        "  - **Precision:** How many predicted spams are actually spam  \n",
        "  - **Recall:** How many actual spams are correctly identified  \n",
        "  - **F1-score:** Balance between precision and recall  \n",
        "  - **ROC-AUC:** Measures overall discriminative ability of the classifier  \n",
        "\n",
        "\n",
        "\n",
        "###  Business Impact\n",
        "- A robust spam detection system will:  \n",
        "  - Reduce time wasted on unwanted emails  \n",
        "  - Protect users from phishing and malicious attacks  \n",
        "  - Improve overall trust in the company’s communication system  \n",
        "  - Enhance productivity and security for both employees and customers  \n"
      ],
      "metadata": {
        "id": "B08k03xUqmm1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Question 10: Imagine you’re working as a data scientist for a company that handles\n",
        "email communications.\n",
        "Your task is to automatically classify emails as Spam or Not Spam. The emails may\n",
        "contain:\n",
        "● Text with diverse vocabulary\n",
        "● Potential class imbalance (far more legitimate emails than spam)\n",
        "● Some incomplete or missing data\n",
        "Explain the approach you would take to:\n",
        "● Preprocess the data (e.g. text vectorization, handling missing data)\n",
        "● Choose and justify an appropriate model (SVM vs. Naïve Bayes)\n",
        "● Address class imbalance\n",
        "● Evaluate the performance of your solution with suitable metrics\n",
        "And explain the business impact of your solution.\n",
        "\"\"\"\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import classification_report\n",
        "import numpy as np\n",
        "\n",
        "# Synthetic email dataset (you’d replace with real emails)\n",
        "emails = [\"Win money now!!!\", \"Meeting at 10am\", \"Lowest price on meds\", \"Project deadline tomorrow\", \"Earn $$$ fast\"]\n",
        "labels = [1, 0, 1, 0, 1]  # 1=Spam, 0=Not Spam\n",
        "\n",
        "# Vectorization\n",
        "vectorizer = TfidfVectorizer(stop_words='english')\n",
        "X = vectorizer.fit_transform(emails)\n",
        "y = np.array(labels)\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train NB\n",
        "model = MultinomialNB()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate\n",
        "y_pred = model.predict(X_test)\n",
        "print(classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HRiXvVSgqBN6",
        "outputId": "bd413975-1e3d-44f5-f7f7-15ab7f20bb5f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00         1\n",
            "           1       0.50      1.00      0.67         1\n",
            "\n",
            "    accuracy                           0.50         2\n",
            "   macro avg       0.25      0.50      0.33         2\n",
            "weighted avg       0.25      0.50      0.33         2\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GhgyeKiqqiq-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}